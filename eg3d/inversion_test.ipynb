{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"./eg3d/eg3d\" not in sys.path:\n",
    "    sys.path.append(\"./eg3d/eg3d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_utils import custom_ops\n",
    "custom_ops.verbosity = \"full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")  # TODO\n",
    "with open(\"./eg3d/eg3d/networks/ffhqrebalanced512-128.pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)[\"G_ema\"].cuda()  # torch.nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def w_projection(*, G, target, num_steps=1000, w_avg_samples=10000,):\n",
    "  G = copy.deepcopy(G).eval().requires_grad_(False).to(device).float() \n",
    "\n",
    "  z_samples = np.random.RandomState(123).randn(w_avg_samples, G.z_dim)\n",
    "  w_samples = G.mapping(torch.from_numpy(z_samples).to(device), None)  # [N, L, C]\n",
    "  w_samples = w_samples[:, :1, :].cpu().numpy().astype(np.float32)  # [N, 1, C]\n",
    "  w_avg = np.mean(w_samples, axis=0, keepdims=True)  # [1, 1, C]\n",
    "  w_avg_tensor = torch.from_numpy(w_avg).to(global_config.device)\n",
    "  w_std = (np.sum((w_samples - w_avg) ** 2) / w_avg_samples) ** 0.5\n",
    "\n",
    "  start_w = w_avg\n",
    "\n",
    "  # Setup noise inputs.\n",
    "  noise_bufs = {name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load('/home/aiteam/tykim/generative/gan/pose/ml-gmpi/ckpts/gmpi_pretrained/FFHQ1024/ema.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['num_updates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project(\n",
    "        G,\n",
    "        target: torch.Tensor,  # [C,H,W] and dynamic range [0,255], W & H must match G output resolution\n",
    "        *,\n",
    "        num_steps=1000,\n",
    "        w_avg_samples=10000,\n",
    "        initial_learning_rate=0.01,\n",
    "        initial_noise_factor=0.05,\n",
    "        lr_rampdown_length=0.25,\n",
    "        lr_rampup_length=0.05,\n",
    "        noise_ramp_length=0.75,\n",
    "        regularize_noise_weight=1e5,\n",
    "        verbose=False,\n",
    "        device: torch.device,\n",
    "        use_wandb=False,\n",
    "        initial_w=None,\n",
    "        image_log_step=global_config.image_rec_result_log_snapshot,\n",
    "        w_name: str\n",
    "):\n",
    "    assert target.shape == (G.img_channels, G.img_resolution, G.img_resolution)\n",
    "\n",
    "    def logprint(*args):\n",
    "        if verbose:\n",
    "            print(*args)\n",
    "\n",
    "    G = copy.deepcopy(G).eval().requires_grad_(False).to(device).float()  # type: ignore\n",
    "\n",
    "    # Compute w stats.\n",
    "    logprint(f'Computing W midpoint and stddev using {w_avg_samples} samples...')\n",
    "    z_samples = np.random.RandomState(123).randn(w_avg_samples, G.z_dim)\n",
    "    w_samples = G.mapping(torch.from_numpy(z_samples).to(device), None)  # [N, L, C]\n",
    "    w_samples = w_samples[:, :1, :].cpu().numpy().astype(np.float32)  # [N, 1, C]\n",
    "    w_avg = np.mean(w_samples, axis=0, keepdims=True)  # [1, 1, C]\n",
    "    w_avg_tensor = torch.from_numpy(w_avg).to(global_config.device)\n",
    "    w_std = (np.sum((w_samples - w_avg) ** 2) / w_avg_samples) ** 0.5\n",
    "\n",
    "    start_w = initial_w if initial_w is not None else w_avg\n",
    "\n",
    "    # Setup noise inputs.\n",
    "    noise_bufs = {name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name}\n",
    "\n",
    "    # Load VGG16 feature detector.\n",
    "    url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/vgg16.pt'\n",
    "    with dnnlib.util.open_url(url) as f:\n",
    "        vgg16 = torch.jit.load(f).eval().to(device)\n",
    "\n",
    "    # Features for target image.\n",
    "    target_images = target.unsqueeze(0).to(device).to(torch.float32)\n",
    "    if target_images.shape[2] > 256:\n",
    "        target_images = F.interpolate(target_images, size=(256, 256), mode='area')\n",
    "    target_features = vgg16(target_images, resize_images=False, return_lpips=True)\n",
    "\n",
    "    w_opt = torch.tensor(start_w, dtype=torch.float32, device=device,\n",
    "                         requires_grad=True)  # pylint: disable=not-callable\n",
    "    optimizer = torch.optim.Adam([w_opt] + list(noise_bufs.values()), betas=(0.9, 0.999),\n",
    "                                 lr=hyperparameters.first_inv_lr)\n",
    "\n",
    "    # Init noise.\n",
    "    for buf in noise_bufs.values():\n",
    "        buf[:] = torch.randn_like(buf)\n",
    "        buf.requires_grad = True\n",
    "\n",
    "    for step in tqdm(range(num_steps)):\n",
    "\n",
    "        # Learning rate schedule.\n",
    "        t = step / num_steps\n",
    "        w_noise_scale = w_std * initial_noise_factor * max(0.0, 1.0 - t / noise_ramp_length) ** 2\n",
    "        lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n",
    "        lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n",
    "        lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n",
    "        lr = initial_learning_rate * lr_ramp\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        # Synth images from opt_w.\n",
    "        w_noise = torch.randn_like(w_opt) * w_noise_scale\n",
    "        ws = (w_opt + w_noise).repeat([1, G.mapping.num_ws, 1])\n",
    "        synth_images = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "\n",
    "        # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n",
    "        synth_images = (synth_images + 1) * (255 / 2)\n",
    "        if synth_images.shape[2] > 256:\n",
    "            synth_images = F.interpolate(synth_images, size=(256, 256), mode='area')\n",
    "\n",
    "        # Features for synth images.\n",
    "        synth_features = vgg16(synth_images, resize_images=False, return_lpips=True)\n",
    "        dist = (target_features - synth_features).square().sum()\n",
    "\n",
    "        # Noise regularization.\n",
    "        reg_loss = 0.0\n",
    "        for v in noise_bufs.values():\n",
    "            noise = v[None, None, :, :]  # must be [1,1,H,W] for F.avg_pool2d()\n",
    "            while True:\n",
    "                reg_loss += (noise * torch.roll(noise, shifts=1, dims=3)).mean() ** 2\n",
    "                reg_loss += (noise * torch.roll(noise, shifts=1, dims=2)).mean() ** 2\n",
    "                if noise.shape[2] <= 8:\n",
    "                    break\n",
    "                noise = F.avg_pool2d(noise, kernel_size=2)\n",
    "        loss = dist + reg_loss * regularize_noise_weight\n",
    "\n",
    "        if step % image_log_step == 0:\n",
    "            with torch.no_grad():\n",
    "                if use_wandb:\n",
    "                    global_config.training_step += 1\n",
    "                    wandb.log({f'first projection _{w_name}': loss.detach().cpu()}, step=global_config.training_step)\n",
    "                    log_utils.log_image_from_w(w_opt.repeat([1, G.mapping.num_ws, 1]), G, w_name)\n",
    "\n",
    "        # Step\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        logprint(f'step {step + 1:>4d}/{num_steps}: dist {dist:<4.2f} loss {float(loss):<5.2f}')\n",
    "\n",
    "        # Normalize noise.\n",
    "        with torch.no_grad():\n",
    "            for buf in noise_bufs.values():\n",
    "                buf -= buf.mean()\n",
    "                buf *= buf.square().mean().rsqrt()\n",
    "\n",
    "    del G\n",
    "    return w_opt.repeat([1, 18, 1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('eg3d')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0757d6925bc23c2f30026c6f10c42ba2226ac99bb9ce241ac526661745f2cf6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
