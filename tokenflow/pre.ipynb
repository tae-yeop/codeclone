{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_gradient(x, y):\n",
    "    # Even though we're inside a no_grad context, we can still compute gradients using autograd.grad\n",
    "    grad_x = torch.autograd.grad(y, x, grad_outputs=torch.ones_like(y), retain_graph=True, create_graph=True)[0]\n",
    "    return grad_x\n",
    "\n",
    "# Define a tensor with requires_grad=True\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "gradient = compute_gradient(x, y)\n",
    "print(gradient)  # This should print tensor([4.], grad_fn=<MulBackward0>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer, logging\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_key = \"runwayml/stable-diffusion-v1-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_scheduler = DDIMScheduler.from_pretrained(model_key, subfolder=\"scheduler\", cache_dir='/home/tyk/hf_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDIMScheduler {\n",
       "  \"_class_name\": \"DDIMScheduler\",\n",
       "  \"_diffusers_version\": \"0.20.2\",\n",
       "  \"beta_end\": 0.012,\n",
       "  \"beta_schedule\": \"scaled_linear\",\n",
       "  \"beta_start\": 0.00085,\n",
       "  \"clip_sample\": false,\n",
       "  \"clip_sample_range\": 1.0,\n",
       "  \"dynamic_thresholding_ratio\": 0.995,\n",
       "  \"num_train_timesteps\": 1000,\n",
       "  \"prediction_type\": \"epsilon\",\n",
       "  \"rescale_betas_zero_snr\": false,\n",
       "  \"sample_max_value\": 1.0,\n",
       "  \"set_alpha_to_one\": false,\n",
       "  \"skip_prk_steps\": true,\n",
       "  \"steps_offset\": 1,\n",
       "  \"thresholding\": false,\n",
       "  \"timestep_spacing\": \"leading\",\n",
       "  \"trained_betas\": null\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy_scheduler.timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDIMScheduler {\n",
       "  \"_class_name\": \"DDIMScheduler\",\n",
       "  \"_diffusers_version\": \"0.20.2\",\n",
       "  \"beta_end\": 0.012,\n",
       "  \"beta_schedule\": \"scaled_linear\",\n",
       "  \"beta_start\": 0.00085,\n",
       "  \"clip_sample\": false,\n",
       "  \"clip_sample_range\": 1.0,\n",
       "  \"dynamic_thresholding_ratio\": 0.995,\n",
       "  \"num_train_timesteps\": 1000,\n",
       "  \"prediction_type\": \"epsilon\",\n",
       "  \"rescale_betas_zero_snr\": false,\n",
       "  \"sample_max_value\": 1.0,\n",
       "  \"set_alpha_to_one\": false,\n",
       "  \"skip_prk_steps\": true,\n",
       "  \"steps_offset\": 1,\n",
       "  \"thresholding\": false,\n",
       "  \"timestep_spacing\": \"leading\",\n",
       "  \"trained_betas\": null\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_scheduler.set_timesteps(70)\n",
    "toy_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([967, 953, 939, 925, 911, 897, 883, 869, 855, 841, 827, 813, 799, 785,\n",
       "        771, 757, 743, 729, 715, 701, 687, 673, 659, 645, 631, 617, 603, 589,\n",
       "        575, 561, 547, 533, 519, 505, 491, 477, 463, 449, 435, 421, 407, 393,\n",
       "        379, 365, 351, 337, 323, 309, 295, 281, 267, 253, 239, 225, 211, 197,\n",
       "        183, 169, 155, 141, 127, 113,  99,  85,  71,  57,  43,  29,  15,   1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_scheduler.timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,  15,  29,  43,  57,  71,  85,  99, 113, 127, 141, 155, 169, 183,\n",
       "        197, 211, 225, 239, 253, 267, 281, 295, 309, 323, 337, 351, 365, 379,\n",
       "        393, 407, 421, 435, 449, 463, 477, 491, 505, 519, 533, 547, 561, 575,\n",
       "        589, 603, 617, 631, 645, 659, 673, 687, 701, 715, 729, 743, 757, 771,\n",
       "        785, 799, 813, 827, 841, 855, 869, 883, 897, 911, 925, 939, 953, 967])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timesteps = reversed(toy_scheduler.timesteps)\n",
    "timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([967, 953, 939, 925, 911, 897, 883, 869, 855, 841, 827, 813, 799, 785,\n",
       "        771, 757, 743, 729, 715, 701, 687, 673, 659, 645, 631, 617, 603, 589,\n",
       "        575, 561, 547, 533, 519, 505, 491, 477, 463, 449, 435, 421, 407, 393,\n",
       "        379, 365, 351, 337, 323, 309, 295, 281, 267, 253, 239, 225, 211, 197,\n",
       "        183, 169, 155, 141, 127, 113,  99,  85,  71,  57,  43,  29,  15,   1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_scheduler.timesteps[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_timestep = min(int(70 * 1.0), 70)\n",
    "init_timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_start = max(70 - init_timestep, 0)\n",
    "t_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timesteps(scheduler, num_inference_steps, strength, device):\n",
    "    # get the original timestep using init_timestep\n",
    "    init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n",
    "\n",
    "    t_start = max(num_inference_steps - init_timestep, 0)\n",
    "    timesteps = scheduler.timesteps[t_start:]\n",
    "\n",
    "    return timesteps, num_inference_steps - t_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_path = 'test'\n",
    "n_frames = 70\n",
    "# [f\"{frames_path}/%05d.png\" % i for i in range(n_frames)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps_to_save =  timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1)\n",
      "1 tensor(15)\n",
      "2 tensor(29)\n",
      "3 tensor(43)\n",
      "4 tensor(57)\n",
      "5 tensor(71)\n",
      "6 tensor(85)\n",
      "7 tensor(99)\n",
      "8 tensor(113)\n",
      "9 tensor(127)\n",
      "10 tensor(141)\n",
      "11 tensor(155)\n",
      "12 tensor(169)\n",
      "13 tensor(183)\n",
      "14 tensor(197)\n",
      "15 tensor(211)\n",
      "16 tensor(225)\n",
      "17 tensor(239)\n",
      "18 tensor(253)\n",
      "19 tensor(267)\n",
      "20 tensor(281)\n",
      "21 tensor(295)\n",
      "22 tensor(309)\n",
      "23 tensor(323)\n",
      "24 tensor(337)\n",
      "25 tensor(351)\n",
      "26 tensor(365)\n",
      "27 tensor(379)\n",
      "28 tensor(393)\n",
      "29 tensor(407)\n",
      "30 tensor(421)\n",
      "31 tensor(435)\n",
      "32 tensor(449)\n",
      "33 tensor(463)\n",
      "34 tensor(477)\n",
      "35 tensor(491)\n",
      "36 tensor(505)\n",
      "37 tensor(519)\n",
      "38 tensor(533)\n",
      "39 tensor(547)\n",
      "40 tensor(561)\n",
      "41 tensor(575)\n",
      "42 tensor(589)\n",
      "43 tensor(603)\n",
      "44 tensor(617)\n",
      "45 tensor(631)\n",
      "46 tensor(645)\n",
      "47 tensor(659)\n",
      "48 tensor(673)\n",
      "49 tensor(687)\n",
      "50 tensor(701)\n",
      "51 tensor(715)\n",
      "52 tensor(729)\n",
      "53 tensor(743)\n",
      "54 tensor(757)\n",
      "55 tensor(771)\n",
      "56 tensor(785)\n",
      "57 tensor(799)\n",
      "58 tensor(813)\n",
      "59 tensor(827)\n",
      "60 tensor(841)\n",
      "61 tensor(855)\n",
      "62 tensor(869)\n",
      "63 tensor(883)\n",
      "64 tensor(897)\n",
      "65 tensor(911)\n",
      "66 tensor(925)\n",
      "67 tensor(939)\n",
      "68 tensor(953)\n",
      "69 tensor(967)\n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(timesteps):\n",
    "    print(i, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Preprocess(nn.Module):\n",
    "    def __init__(self, device, opt, hf_key=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.sd_version = opt.sd_version\n",
    "        self.use_depth = False\n",
    "\n",
    "        self.model_key = model_key\n",
    "        self.vae = AutoencoderKL.from_pretrained(model_key, subfolder=\"vae\", revision=\"fp16\",\n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 cache=).to(self.device)\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(model_key, subfolder=\"tokenizer\",\n",
    "                                                       cache=)\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(model_key, subfolder=\"text_encoder\", revision=\"fp16\",\n",
    "                                                          torch_dtype=torch.float16).to(self.device)\n",
    "        self.unet = UNet2DConditionModel.from_pretrained(model_key, subfolder=\"unet\", revision=\"fp16\",\n",
    "                                                   torch_dtype=torch.float16).to(self.device)\n",
    "\n",
    "        self.scheduler = DDIMScheduler.from_pretrained(model_key, subfolder=\"scheduler\")\n",
    "\n",
    "\n",
    "        self.paths, self.frames, self.latents = self.get_data(opt.data_path, opt.n_frames)\n",
    "\n",
    "\n",
    "    def get_data(self, frames_path, n_frames):\n",
    "        # frame data가 있는 위치\n",
    "        # n_frames갯수만큼 path 0~n_frames\n",
    "        paths = [f'{frames_path}/%05d.png' % i for i in range(n_frames)]\n",
    "        if not os.path.exists(paths[0]):\n",
    "            paths = [f\"{frames_path}/%05d.jpg\" % i for i in range(n_frames)]\n",
    "        self.paths = paths\n",
    "\n",
    "        # 이미지 오픈\n",
    "        frames = [Image.open(path).convert('RGB') for path in paths]\n",
    "        if frames[0].size[0] == frames[0].size[1]:\n",
    "            # 사이즈 처리\n",
    "            frames = [frame.resize((512, 512), resample=Image.Resampling.LANCZOS) for frame in frames]\n",
    "        frames = torch.stack([T.ToTensor()(frame) for frame in frames]).to(torch.float16).to(self.device)\n",
    "\n",
    "        latents = self.encode_imgs(frames, deterministic=True).to(torch.float16).to(self.device)\n",
    "        return paths, frames, latents\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_imgs(self, imgs, batch_size=10, deterministic=True):\n",
    "        imgs = 2 * imgs - 1\n",
    "        latents = []\n",
    "        for i in range(0, len(imgs), batch_size):\n",
    "            # vae로 넣음\n",
    "            posterior = self.vae.encode(imgs[i:i + batch_size]).latent_dist\n",
    "            latent = posterior.mean if deterministic else posterior.sample()\n",
    "            latents.append(latent * 0.18215)\n",
    "        latents = torch.cat(latents)\n",
    "        return latents\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract_latents(self,\n",
    "                        num_steps,\n",
    "                        save_path,\n",
    "                        batch_size,\n",
    "                        timesteps_to_save,\n",
    "                        inversion_prompt=''):\n",
    "        self.scheduler.set_timesteps(num_steps)\n",
    "        cond = self.get_text_embeds()\n",
    "        latent_frames = self.latents\n",
    "\n",
    "        inverted_x = self.ddim_inversion(cond,\n",
    "                                         latent_frames,\n",
    "                                         save_path,\n",
    "                                         batch_size=batch_size,\n",
    "                                         save_latents=True,\n",
    "                                         timesteps_to_save=timesteps_to_save)\n",
    "        latent_reconstruction = self.ddim_sample(inverted_x, cond, batch_size=batch_size)\n",
    "        \n",
    "        rgb_reconstruction = self.decode_latents(latent_reconstruction)\n",
    "        return rgb_reconstruction\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_sample(self, x, cond, batch_size):\n",
    "        timesteps = self.scheduler.timesteps\n",
    "        for i, t in enumerate(tqdm(timesteps)):\n",
    "            for b in range(0, x.shape[0], batch_size):\n",
    "                x_batch = x[b: b+batch_size]\n",
    "                model_input = x_batch\n",
    "                cond_batch = cond.repeat(x_batch.shape[0], 1, 1)\n",
    "\n",
    "                alpha_prod_t = self.scheduler.alphas_cumprod[t]\n",
    "                alpha_prod_t_prev = (self.scheduler.alphas_cumprod[timesteps[i+1]]\n",
    "                                     if i < len(timesteps) - 1\n",
    "                                     else self.scheduler.final_alpha_cumprod)\n",
    "\n",
    "                mu = alpha_prod_t ** 0.5\n",
    "                sigma = (1 - alpha_prod_t) ** 0.5\n",
    "                mu_prev = alpha_prod_t_prev ** 0.5\n",
    "                sigma_prev = (1 - alpha_prod_t_prev) ** 0.5\n",
    "\n",
    "                if self.sd_version != 'ControlNet':\n",
    "                    eps = self.unet(model_input, t, encoder_hidden_states=cond_batch).sample\n",
    "                else:\n",
    "                    eps = self.controlnet_pred(x_batch, t, cond_batch, torch.cat([self.canny_cond[b: b+batch_size]]))\n",
    "\n",
    "                pred_x0 = (x_batch - sigma * eps) / mu\n",
    "                x[b:b+batch_size] = mu_prev * pred_x0 + sigma_prev * eps\n",
    "\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def decode_latents(self, latents):\n",
    "        decoded = []\n",
    "        batch_size = 8\n",
    "        for b in range(0, latents.shape[0], batch_size):\n",
    "            latents_batch = 1 /  0.18215 * latents[b:b + batch_size]\n",
    "            imgs = self.vae.decode(latents_batch).sample\n",
    "            imgs = (imgs / 2 + 0.5).clamp(0, 1)\n",
    "            decoded.append(imgs)\n",
    "        return torch.cat(decoded)\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_inversion(self, cond, ):\n",
    "        timesteps = reversed(self.scheduler.timesteps)\n",
    "        timesteps_to_save = timesteps_to_save if timesteps_to_save is not None else timesteps\n",
    "        for i, t in enumerate(timesteps):\n",
    "            # 0부터 1000 순으로 됨\n",
    "            for b in range(0, latent_frames.shape[0], batch_size):\n",
    "                x_batch = latent_frames[b:b + batch_size]\n",
    "                model_input = x_batch\n",
    "                cond_batch = cond.repeat()\n",
    "\n",
    "                alpha_prod_t = self.scheduler.alphas_cumprod[t]\n",
    "                alpha_prod_t_prev = (self.scheduler.alphas_cumprod[timesteps[i-1]]\n",
    "                                     if i > 0 else self.scheduler.final_alphas_cumprod)\n",
    "\n",
    "                mu = alpha_prod_t ** 0.5\n",
    "                mu_prev = alpha_prod_t_prev ** 0.5\n",
    "                sigma = (1 - alpha_prod_t) ** 0.5\n",
    "                sigma_prev = (1 - alpha_prod_t_prev) ** 0.5\n",
    "\n",
    "                if self.sd_version != 'ControlNet':\n",
    "                    eps = self.unet(model_input, t, encoder_hidden_states=cond_batch).sample\n",
    "                else:\n",
    "                    eps = self.controlnet_pred(x_batch, t, cond_batch, torch.cat([self.canny_cond[b: b + batch_size]]))\n",
    "\n",
    "                pred_x0 = (x_batch - sigma_prev * eps) / mu_prev\n",
    "                latent_frames[b:b + batch_size] = mu * pred_x0 + sigma * eps\n",
    "\n",
    "            if save_latents and t in timesteps_to_save:\n",
    "                torch.save(latent_frames, os.path.join(save_path, 'latents', f'noisy_latents_{t}.pt'))\n",
    "        torch.save(latent_frames, os.path.join(save_path, 'latents', f'noisy_latents_{t}.pt'))\n",
    "        return latent_frames"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
