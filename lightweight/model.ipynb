{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_wavelets import DWTInverse, DWTForward\n",
    "\n",
    "from math import floor, ceil\n",
    "from einops import rearrange\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "from PIL import Image\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "img = Image.open('/home/aiteam/tykim/generative/gan/2291_04.png')\n",
    "from torchvision.transforms import ToTensor,ToPILImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn(in_channels, out_channels, stride, padding, groups=1):\n",
    "    result = nn.Sequential()\n",
    "    result.add_module('conv3x3', nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                                        kernel_size=3, stride=stride, padding=padding, groups=groups,bias=False))\n",
    "    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))\n",
    "    result.add_module('conv1x1', nn.Conv2d(in_channels=out_channels, out_channels=out_channels, \n",
    "                                        kernel_size=1, stride=stride, padding=padding, groups=groups,bias=False))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_module(module):\n",
    "    \"\"\"\n",
    "    Zero out the parameters of a module and return it.\n",
    "    \"\"\"\n",
    "    for p in module.parameters():\n",
    "        p.detach().zero_()\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = conv_bn(3, 10, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[1, 10, 34, 34] n=11560 x∈[-2.491, 2.663] μ=-1.196e-09 σ=0.577 grad ConvolutionBackward0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(torch.randn(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, \n",
    "                 inference_mode=False, stride=1, padding=0, dilation=1,\n",
    "                 groups=1, num_conv_branches=1, zero_params=False):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.inference_mode = inference_mode\n",
    "        self.stride = stride\n",
    "        self.groups = groups\n",
    "        self.num_conv_branches = num_conv_branches\n",
    "\n",
    "        \n",
    "        self.se = nn.Identity()\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        if inference_mode:\n",
    "            self.reparam_conv = nn.Conv2d(in_channels=in_channels,\n",
    "                                          out_channels=out_channels,\n",
    "                                          kernel_size=kernel_size,\n",
    "                                          stride=stride,\n",
    "                                          padding=padding,\n",
    "                                          dilation=dilation,\n",
    "                                          groups=groups,\n",
    "                                          bias=True)\n",
    "        else:\n",
    "            # Re-parameterizable skip connection\n",
    "            self.rbr_skip = nn.BatchNorm2d(num_features=in_channels) \\\n",
    "                if out_channels == in_channels and stride == 1 else None\n",
    "\n",
    "            # Re-parameterizable conv branches\n",
    "            rbr_conv = list()\n",
    "            for _ in range(self.num_conv_branches):\n",
    "                rbr_conv.append(self._conv_bn(kernel_size=kernel_size,\n",
    "                                              padding=padding))\n",
    "            self.rbr_conv = nn.ModuleList(rbr_conv)\n",
    "\n",
    "            # Re-parameterizable scale branch\n",
    "            self.rbr_scale = None\n",
    "            if kernel_size > 1:\n",
    "                self.rbr_scale = self._conv_bn(kernel_size=1,\n",
    "                                               padding=0)\n",
    "\n",
    "            if zero_params:\n",
    "                zero_module(self.rbr_skip) if self.rbr_skip is not None else ...\n",
    "                zero_module(self.rbr_conv) \n",
    "                zero_module(self.rbr_scale) if self.rbr_scale is not None else ...\n",
    "    def forward(self, x):\n",
    "        # Inference mode forward pass.\n",
    "        if self.inference_mode:\n",
    "            return self.activation(self.se(self.reparam_conv(x)))\n",
    "\n",
    "        # Multi-branched train-time forward pass.\n",
    "        # Skip branch output\n",
    "        identity_out = 0\n",
    "        if self.rbr_skip is not None:\n",
    "            identity_out = self.rbr_skip(x)\n",
    "\n",
    "        # Scale branch output\n",
    "        scale_out = 0\n",
    "        if self.rbr_scale is not None:\n",
    "            scale_out = self.rbr_scale(x)\n",
    "\n",
    "        # Other branches\n",
    "        out = scale_out + identity_out\n",
    "        for ix in range(self.num_conv_branches):\n",
    "            out += self.rbr_conv[ix](x)\n",
    "\n",
    "        return self.activation(self.se(out))\n",
    "    def _conv_bn(self,\n",
    "                 kernel_size: int,\n",
    "                 padding: int) -> nn.Sequential:\n",
    "        \"\"\" Helper method to construct conv-batchnorm layers.\n",
    "\n",
    "        :param kernel_size: Size of the convolution kernel.\n",
    "        :param padding: Zero-padding size.\n",
    "        :return: Conv-BN module.\n",
    "        \"\"\"\n",
    "        mod_list = nn.Sequential()\n",
    "        mod_list.add_module('conv', nn.Conv2d(in_channels=self.in_channels,\n",
    "                                              out_channels=self.out_channels,\n",
    "                                              kernel_size=kernel_size,\n",
    "                                              stride=self.stride,\n",
    "                                              padding=padding,\n",
    "                                              groups=self.groups,\n",
    "                                              bias=False))\n",
    "        mod_list.add_module('bn', nn.BatchNorm2d(num_features=self.out_channels))\n",
    "        return mod_list\n",
    "    \n",
    "    def _fuse_bn_tensor(self, branch) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\" Method to fuse batchnorm layer with preceeding conv layer.\n",
    "        Reference: https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py#L95\n",
    "\n",
    "        :param branch:\n",
    "        :return: Tuple of (kernel, bias) after fusing batchnorm.\n",
    "        \"\"\"\n",
    "        if isinstance(branch, nn.Sequential):\n",
    "            kernel = branch.conv.weight\n",
    "            running_mean = branch.bn.running_mean\n",
    "            running_var = branch.bn.running_var\n",
    "            gamma = branch.bn.weight\n",
    "            beta = branch.bn.bias\n",
    "            eps = branch.bn.eps\n",
    "        else:\n",
    "            assert isinstance(branch, nn.BatchNorm2d)\n",
    "            if not hasattr(self, 'id_tensor'):\n",
    "                input_dim = self.in_channels // self.groups\n",
    "                kernel_value = torch.zeros((self.in_channels,\n",
    "                                            input_dim,\n",
    "                                            self.kernel_size,\n",
    "                                            self.kernel_size),\n",
    "                                           dtype=branch.weight.dtype,\n",
    "                                           device=branch.weight.device)\n",
    "                for i in range(self.in_channels):\n",
    "                    kernel_value[i, i % input_dim,\n",
    "                                 self.kernel_size // 2,\n",
    "                                 self.kernel_size // 2] = 1\n",
    "                self.id_tensor = kernel_value\n",
    "            kernel = self.id_tensor\n",
    "            running_mean = branch.running_mean\n",
    "            running_var = branch.running_var\n",
    "            gamma = branch.weight\n",
    "            beta = branch.bias\n",
    "            eps = branch.eps\n",
    "        std = (running_var + eps).sqrt()\n",
    "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
    "        return kernel * t, beta - running_mean * gamma / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = RepBlock(in_channels=3, out_channels=16, kernel_size=3, inference_mode=False, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[1, 16, 32, 32] n=16384 x∈[0., 5.577] μ=0.560 σ=0.820 grad ReluBackward0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep(torch.randn(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep2 = RepBlock(in_channels=3, out_channels=16, kernel_size=3, inference_mode=True, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[1, 16, 32, 32] n=16384 x∈[0., 2.215] μ=0.252 σ=0.346 grad ReluBackward0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep2(torch.randn(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(rep2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_wavelets import DWTInverse, DWTForward\n",
    "\n",
    "dwt = DWTForward(J=1, mode='zero', wave='db1')\n",
    "idwt = DWTInverse(mode=\"zero\", wave=\"db1\")\n",
    "\n",
    "def img_to_dwt(img):\n",
    "    low, high = dwt(img)\n",
    "    b, _, _, h, w = high[0].size()\n",
    "    high = high[0].view(b, -1, h, w)\n",
    "    freq = torch.cat([low, high], dim=1)\n",
    "    return freq\n",
    "\n",
    "def dwt_to_img(img):\n",
    "    b, c, h, w = img.size()\n",
    "    low = img[:, :3, :, :]\n",
    "    high = img[:, 3:, :, :].view(b, 3, 3, h, w)\n",
    "    return idwt((low, [high]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsample(nn.Module):\n",
    "    def __init__(self, channels, out_channels=None, kernel=3, stride=None, padding=1): # use_conv,\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        # self.use_conv = use_conv\n",
    "        stride = 2 if stride is None else stride\n",
    "\n",
    "        # if use_conv:\n",
    "        self.op = nn.Conv2d(channels, self.out_channels, kernel_size=kernel, stride=stride, padding=padding)\n",
    "        # else:\n",
    "        #     self.op = DWTForward(J=1, mode='zero', wave='db1')\n",
    "\n",
    "    # def img_to_dwt(self, img):\n",
    "    #     low, high = self.op(img)\n",
    "    #     b, _, _, h, w = high[0].size()\n",
    "    #     high = high[0].view(b, -1, h, w)\n",
    "    #     freq = torch.cat([low, high], dim=1)\n",
    "    #     return freq\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        x = self.op(x)\n",
    "        return x\n",
    "        # assert x.shape[1] == self.channels\n",
    "        # if self.use_conv:\n",
    "        #     x = self.op(x)\n",
    "        # else:\n",
    "        #     x = self.img_to_dwt(x)\n",
    "        # return x\n",
    "            \n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, channels, out_channels=None, paddding=1): # use_conv,\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        # self.use_conv = use_conv\n",
    "        # if use_conv:\n",
    "        self.op = nn.Conv2d(self.channels, self.out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        # else:\n",
    "        #     self.op = DWTInverse(mode=\"zero\", wave=\"db1\")\n",
    "            \n",
    "    # def dwt_to_img(self, img):\n",
    "    #     b, c, h, w = img.size()\n",
    "    #     low = img[:, :3, :, :]\n",
    "    #     high = img[:, 3:, :, :].view(b, 3, 3, h, w)\n",
    "    #     return self.op((low, [high]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        # if self.use_conv:\n",
    "        #     x = F.interpolate(x, scale_factor=2, mode='bicubic')\n",
    "        #     x = self.op(x)\n",
    "        # else:\n",
    "        #     x = self.dwt_to_img(x)\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bicubic')\n",
    "        x = self.op(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = DWTForward(J=1, mode='zero', wave='haar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "low, high = test(torch.randn(1, 13, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "LH, HL, HH = tuple(rearrange(high[0] , 'b c n h w -> n b c h w ', n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 128, 128])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LH.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 128, 128])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[1, 13, 3, 128, 128] n=638976 x∈[-4.878, 4.881] μ=0.000 σ=1.000"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[1, 13, 4, 128, 128] n=851968 x∈[-5.172, 4.949] μ=-0.001 σ=0.873"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat((low.view(1, 13, 1, 128, 128), high[0]), dim=2) * torch.randn(1, 4, 1, 1).view(1, -1, 4, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[1, 13, 3, 128, 128] n=638976 x∈[-4.878, 4.881] μ=0.000 σ=1.000"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 128, 128])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "LH, HL, HH = tuple(rearrange(high[0] , 'b c n h w -> n b c h w ', n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, _, _, h, w = high[0].size()\n",
    "high = high[0].view(b, -1, h, w)\n",
    "freq = torch.cat([low, high], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 128, 128])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = DWTInverse(mode=\"zero\", wave=\"haar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 128, 128])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dwt_to_img(img):\n",
    "    b, c, h, w = img.size()\n",
    "    low = img[:, :3, :, :]\n",
    "    high = img[:, 3:, :, :].view(b, 3, 3, h, w)\n",
    "    return op((low, [high]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[1, 3, 256, 256] n=196608 x∈[-4.776, 4.699] μ=-0.003 σ=1.001"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dwt_to_img(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgpool = nn.AvgPool2d(kernel_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgpool(torch.randn(1, 3, 256, 256)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 3]' is invalid for input of size 12",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m avgpool \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mAdaptiveAvgPool2d(\u001b[39m2\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m avgpool(torch\u001b[39m.\u001b[39;49mrandn(\u001b[39m1\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m128\u001b[39;49m, \u001b[39m128\u001b[39;49m))\u001b[39m.\u001b[39;49mview(\u001b[39m1\u001b[39;49m, \u001b[39m3\u001b[39;49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 3]' is invalid for input of size 12"
     ]
    }
   ],
   "source": [
    "avgpool = nn.AdaptiveAvgPool2d(2)\n",
    "avgpool(torch.randn(1, 3, 128, 128)).view(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostomAdaptiveAvgPool2D(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        H_in,  W_in  = x.shape[2:]\n",
    "        H_out, W_out = [self.output_size, self.output_size] \\\n",
    "                       if isinstance(self.output_size, int) \\\n",
    "                       else self.output_size\n",
    "        \n",
    "        out_i = []\n",
    "        for i in range(H_out):\n",
    "            out_j = []\n",
    "            for j in range(W_out):\n",
    "                \n",
    "                hs = int(floor(i * H_in / H_out))\n",
    "                he = int(ceil((i+1) * H_in / H_out))\n",
    "                \n",
    "                ws = int(floor(j * W_in / W_out))\n",
    "                we = int(ceil((j+1) * W_in / W_out))\n",
    "                \n",
    "                # print(hs, he, ws, we)\n",
    "                kernel_size = [he-hs, we-ws]\n",
    "                \n",
    "                out = F.avg_pool2d(x[:, :, hs:he, ws:we], kernel_size) \n",
    "                out_j.append(out)\n",
    "                \n",
    "            out_j = torch.concat(out_j, -1)\n",
    "            out_i.append(out_j)\n",
    "            \n",
    "        out_i = torch.concat(out_i, -2)\n",
    "        return out_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveletGating(nn.Module):\n",
    "    def __init__(self, channels, pool_size=4):\n",
    "        super().__init__()\n",
    "        self.pool_size = pool_size\n",
    "        # self.avgpool = nn.AvgPool2d(self.pool_size)\n",
    "        self.avgpool = CostomAdaptiveAvgPool2D(1)\n",
    "        self.fc = nn.Sequential(nn.Conv2d(channels, channels //2, kernel_size=1, bias=False),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Conv2d(channels //2, 4, kernel_size=1, bias=False),\n",
    "                                nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class WGDown(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.dwt = DWTForward(J=1, wave='haar')\n",
    "        self.wavelet_gating = WaveletGating(channels)\n",
    "        \n",
    "    def img_to_dwt(self, img):\n",
    "        low, high = self.dwt(img)\n",
    "        b, _, _, h, w = high[0].size()\n",
    "        high = high[0].view(b, -1, h, w)\n",
    "        freq = torch.cat([low, high], dim=1)\n",
    "        return freq\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : [B, C_in, H, W]\n",
    "        Returns:\n",
    "            [B, C_in, H//2, W//2]\n",
    "        \"\"\"\n",
    "        # dwt를 하고 나오는 순서 : LL, LH, HL, HH\n",
    "        # LL, high = self.dwt(x)\n",
    "        # LH, HL, HH = tuple(rearrange(high[0] , 'b c n h w -> n b c h w ', n=3))\n",
    "        # b, _, _, h, w = high[0].size()\n",
    "        # [1, 4, 1, 1]\n",
    "        # low, high = self.dwt\n",
    "        # score = self.wavelet_gating(x)\n",
    "        # [B, C_in, 4, H//2, W//2]\n",
    "        # result = torch.concat((low.unsqueeze(2), high[0]), dim=2) * score.unsqueeze(1)\n",
    "        # return result.flatten(start_dim=1, end_dim=2)\n",
    "        LL, high = self.dwt(x)\n",
    "        # LH, HL, HH = tuple(rearrange(high[0] , 'b c n h w -> n b c h w ', n=3))\n",
    "        score = self.wavelet_gating(x)\n",
    "        # LL = LL * score[:, 0, ...]\n",
    "        # LH = LH * score[:, 1, ...]\n",
    "        # HL = HL * score[:, 2, ...]\n",
    "        # HH = HH * score[:, 3, ...]\n",
    "        # result = LL + LH + HL + HH\n",
    "        # Stack along a new dimension\n",
    "        # tensors = torch.stack([LL, LH, HL, HH], dim=1) # tensors shape: [B, 4, C_in, H//2, W//2]\n",
    "        # print(low.unsqueeze(2).shape)\n",
    "        # print(high[0])\n",
    "        result = torch.concat((LL.unsqueeze(2), high[0]), dim=2) * score.unsqueeze(1)\n",
    "        result = result.sum(dim=1)\n",
    "        return result\n",
    "\n",
    "class WGUp(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.idwt = DWTInverse(mode=\"zero\", wave='haar')\n",
    "        self.wavelet_gating = WaveletGating(channels)\n",
    "\n",
    "    def dwt_to_img(img):\n",
    "        b, c, h, w = img.size()\n",
    "        low = img[:, :3, :, :]\n",
    "        high = img[:, 3:, :, :].view(b, 3, 3, h, w)\n",
    "        return op((low, [high]))\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : [B, C_in, H, W]\n",
    "        Returns:\n",
    "            [B, C_in, 2*H, 2*W]\n",
    "        \"\"\"\n",
    "        score = self.wavelet_gating(x) # [B, 4, 1, 1]\n",
    "        # x = x.chunk(4, dim=1)\n",
    "        b, c, h, w = x.size()\n",
    "        x_reshaped = x.view(b, 4, c//4, h, w)\n",
    "        # score_reshaped = score.expand(b, 4, c//4, h, w)\n",
    "        freq = x_reshaped * score.unsqueeze(2)\n",
    "        \n",
    "        result = self.idwt((freq[:,0,...], [freq[:, 1:, ...].flatten(start_dim=1, end_dim=2)]))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg = WGUp(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[16, 1, 6, 32, 32] n=98304 x∈[-5.159, 4.541] μ=-0.002 σ=0.999"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(16, 4, 6, 32, 32)[:, :1, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[16, 3, 6, 32, 32] n=294912 x∈[-5.079, 4.457] μ=-0.000 σ=1.000"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(16, 4, 6, 32, 32)[:, 1:, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[257], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m wg(torch\u001b[39m.\u001b[39;49mrandn(\u001b[39m4\u001b[39;49m, \u001b[39m100\u001b[39;49m, \u001b[39m32\u001b[39;49m, \u001b[39m32\u001b[39;49m))\n",
      "File \u001b[0;32m~/miniconda3/envs/3dfm/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[254], line 85\u001b[0m, in \u001b[0;36mWGUp.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m# score_reshaped = score.expand(b, 4, c//4, h, w)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m freq \u001b[39m=\u001b[39m x_reshaped \u001b[39m*\u001b[39m score\u001b[39m.\u001b[39munsqueeze(\u001b[39m2\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49midwt((freq[:,:\u001b[39m1\u001b[39;49m,\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m], [freq[:, \u001b[39m1\u001b[39;49m:, \u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m]\u001b[39m.\u001b[39;49mflatten(start_dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, end_dim\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)]))\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/3dfm/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/3dfm/lib/python3.8/site-packages/pytorch_wavelets/dwt/transform2d.py:146\u001b[0m, in \u001b[0;36mDWTInverse.forward\u001b[0;34m(self, coeffs)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[39mif\u001b[39;00m ll\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m h\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[1;32m    145\u001b[0m         ll \u001b[39m=\u001b[39m ll[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 146\u001b[0m     ll \u001b[39m=\u001b[39m lowlevel\u001b[39m.\u001b[39;49mSFB2D\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m    147\u001b[0m         ll, h, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mg0_col, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mg1_col, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mg0_row, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mg1_row, mode)\n\u001b[1;32m    148\u001b[0m \u001b[39mreturn\u001b[39;00m ll\n",
      "File \u001b[0;32m~/miniconda3/envs/3dfm/lib/python3.8/site-packages/pytorch_wavelets/dwt/lowlevel.py:676\u001b[0m, in \u001b[0;36mSFB2D.forward\u001b[0;34m(ctx, low, highs, g0_row, g1_row, g0_col, g1_col, mode)\u001b[0m\n\u001b[1;32m    673\u001b[0m ctx\u001b[39m.\u001b[39mmode \u001b[39m=\u001b[39m mode\n\u001b[1;32m    674\u001b[0m ctx\u001b[39m.\u001b[39msave_for_backward(g0_row, g1_row, g0_col, g1_col)\n\u001b[0;32m--> 676\u001b[0m lh, hl, hh \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munbind(highs, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    677\u001b[0m lo \u001b[39m=\u001b[39m sfb1d(low, lh, g0_col, g1_col, mode\u001b[39m=\u001b[39mmode, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    678\u001b[0m hi \u001b[39m=\u001b[39m sfb1d(hl, hh, g0_col, g1_col, mode\u001b[39m=\u001b[39mmode, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "wg(torch.randn(4, 100, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor[1, 1, 32, 32] n=1024 x∈[-2.855, 3.688] μ=0.017 σ=0.984,\n",
       " tensor[1, 1, 32, 32] n=1024 x∈[-3.620, 3.164] μ=0.001 σ=0.958,\n",
       " tensor[1, 1, 32, 32] n=1024 x∈[-3.202, 3.188] μ=0.005 σ=0.994,\n",
       " tensor[1, 1, 32, 32] n=1024 x∈[-2.847, 3.910] μ=0.037 σ=0.974)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(torch.randn(1, 4, 32, 32), 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor[1, 1, 32, 32] n=1024 x∈[-3.024, 2.899] μ=-0.035 σ=0.984,\n",
       " tensor[1, 1, 32, 32] n=1024 x∈[-3.229, 2.876] μ=-0.038 σ=0.970,\n",
       " tensor[1, 1, 32, 32] n=1024 x∈[-3.727, 4.430] μ=-0.019 σ=1.021,\n",
       " tensor[1, 1, 32, 32] n=1024 x∈[-3.659, 2.933] μ=0.032 σ=1.029)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1, 4, 32, 32).chunk(4, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgd = WGDown(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 1, 128, 128])\n",
      "tensor[32, 4, 3, 8, 8] n=24576 x∈[-4.129, 3.749] μ=0.005 σ=0.998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4, 8, 8])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgd(torch.randn(32, 4, 16, 16)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[30, 1, 1] x∈[-2.503, 1.897] μ=0.061 σ=1.051"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(30, 4, 1, 1)[:,0,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg = WaveletGating(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[1, 4, 1, 1] x∈[0.500, 0.500] μ=0.500 σ=0.000 grad SigmoidBackward0 [[[[0.500]], [[0.500]], [[0.500]], [[0.500]]]]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wg(torch.randn(1, 10, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "blk = RepBlock(3, 10, 3, padding=1, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 16, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk(torch.randn(1, 3, 32, 32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetModel2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, \n",
    "                 model_channels, num_res_blocks=2, channel_mult=(1,2,4,8),\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.num_resolutions = len(channel_mult)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetModel(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, \n",
    "                 model_channels, num_res_blocks=2, channel_mult=(1,2,4,8), attention_resolutions=[4,2,1], large_kerenl=None, base_kernel=3, freq_domain=False\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.channel_mult = channel_mult\n",
    "        self.num_res_blocks= num_res_blocks\n",
    "\n",
    "        if isinstance(num_res_blocks, int):\n",
    "            self.num_res_blocks = len(channel_mult) * [num_res_blocks] # [2,2,2,2]\n",
    "        else:\n",
    "            if len(num_res_blocks) != len(channel_mult):\n",
    "                raise ValueError(\"provide num_res_blocks either as an int (globally constant) or \"\n",
    "                                 \"as a list/tuple (per-level) with the same length as channel_mult\")\n",
    "            self.num_res_blocks = num_res_blocks\n",
    "\n",
    "        self.base_kernel = base_kernel\n",
    "        self.large_kernel = 7 if large_kerenl is None else base_kernel\n",
    "        self.large_kernel_level = int(len(channel_mult)/2)\n",
    "        \n",
    "        ch = int(channel_mult[0] * model_channels)\n",
    "        input_block_chans = [ch]\n",
    "        stem = [nn.Conv2d(in_channels, ch, kernel_size=3, padding=1)]\n",
    "        if freq_domain:\n",
    "            down = WGDown\n",
    "            up = WGUp\n",
    "        else:\n",
    "            down = Downsample\n",
    "            up = Upsample\n",
    "        # stem.append(WGDown(use_conv=False))\n",
    "        self.input_blocks = nn.ModuleList([nn.Conv2d(in_channels, ch, kernel_size=3, padding=1),])\n",
    "                                           # down(ch)])\n",
    "\n",
    "        # self.input_blocks.append(stem)\n",
    "        self.output_blocks = nn.ModuleList([])\n",
    "\n",
    "        # channel_mult = [1,2,4,4]\n",
    "        # 0, 1\n",
    "        # nr = 2\n",
    "        # 0, 1\n",
    "        # \n",
    "        ds = 1\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            if level <= self.large_kernel_level:\n",
    "                kernel_size = self.large_kernel\n",
    "                padding = 3\n",
    "            else:\n",
    "                kernel_size = self.base_kernel\n",
    "                padding = 1\n",
    "            for nr in range(self.num_res_blocks[level]):\n",
    "                print(nr)\n",
    "                layers = [RepBlock(ch, int(mult * model_channels), kernel_size=kernel_size, padding=padding)]\n",
    "                ch = mult * model_channels\n",
    "                \n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(RepBlock(ch, ch, kernel_size=kernel_size, padding=padding))\n",
    "                \n",
    "                self.input_blocks.append(nn.Sequential(*layers))\n",
    "                input_block_chans.append(ch)\n",
    "            # 0 vs 3\n",
    "            if level != len(channel_mult) -1:\n",
    "                self.input_blocks.append(Downsample(ch))#, use_conv=True))\n",
    "                input_block_chans.append(ch)\n",
    "                ds *= 2\n",
    "            # self.input_blocks.append(nn.Sequential(*layers))\n",
    "        \n",
    "        self.middle_block = nn.Sequential(\n",
    "            RepBlock(ch, ch, kernel_size=3, padding=1),\n",
    "            RepBlock(ch, ch, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        # [(3, 4), (2, 4), (1, 2), (0, 1)]\n",
    "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
    "            if level >= self.large_kernel_level:\n",
    "                kernel_size = self.large_kernel\n",
    "                padding = 3\n",
    "            else:\n",
    "                kernel_size = self.base_kernel\n",
    "                padding = 1\n",
    "            # 0, 1, 2\n",
    "            for i in range(self.num_res_blocks[level] + 1):\n",
    "                ich = input_block_chans.pop()\n",
    "                layers = [RepBlock(ch + ich, model_channels*mult, kernel_size=kernel_size, padding=padding)]\n",
    "                ch = model_channels * mult\n",
    "\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(RepBlock(ch, ch, kernel_size=kernel_size, padding=padding))\n",
    "                if level and i == self.num_res_blocks[level]:\n",
    "                    layers.append(Upsample(ch))#, use_conv=True))\n",
    "                    ds //= 2\n",
    "                self.output_blocks.append(nn.Sequential(*layers))\n",
    "        \n",
    "        self.out = nn.Sequential(RepBlock(model_channels, out_channels, 3, padding=1, zero_params=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        args\n",
    "        x: [N x C x H x W]\n",
    "        returns\n",
    "        an [N x C x H x W]\n",
    "        \"\"\"\n",
    "        hs = []\n",
    "        h = x\n",
    "        for module in self.input_blocks:\n",
    "            h = module(h)\n",
    "            hs.append(h)\n",
    "        h = self.middle_block(h)\n",
    "        # hs = 12\n",
    "        # torch.Size([1, 10, 256, 256])\n",
    "        # torch.Size([1, 10, 256, 256])\n",
    "        # torch.Size([1, 10, 256, 256])\n",
    "        # torch.Size([1, 10, 128, 128])\n",
    "        # torch.Size([1, 20, 128, 128])\n",
    "        # torch.Size([1, 20, 128, 128])\n",
    "        # torch.Size([1, 20, 64, 64])\n",
    "        # torch.Size([1, 40, 64, 64])\n",
    "        # torch.Size([1, 40, 64, 64])\n",
    "        #  torch.Size([1, 40, 32, 32])\n",
    "        # torch.Size([1, 40, 32, 32])\n",
    "        # torch.Size([1, 40, 32, 32])\n",
    "        for module in self.output_blocks:\n",
    "            h = torch.cat([h, hs.pop()], dim=1) # [1, 40, 128, 128] (upsample 때문에) 과 [1, 40, 64, 64]를 concat하려고 함\n",
    "            h = module(h)\n",
    "        return self.out(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "model = UNetModel(3, 3, 10, channel_mult=(1,2,4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.input_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.output_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1699162"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(1, 3, 256, 256)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fused_bn_to_conv_state_dict(conv, bn):\n",
    "    bn_mean, bn_var, bn_gamma, bn_beta = (bn.running_mean, bn.running_var, \n",
    "                                          bn.weight, bn.bias)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward(x, style):\n",
    "    modulatio = sefl.get_modulatin(style)\n",
    "\n",
    "def get_mdoulati0n(sefl,x style):\n",
    "    style = self.modulation(style).view(style.sise(0), -1, 1,1)\n",
    "    modulation = self.scale * style\n",
    "    return modulation\n",
    "\n",
    "def get_demodulation(self, style):\n",
    "    w = self.weight.unsqueeze(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn(in_channels, out_channels, stride, padding, groups=1):\n",
    "    result = nn.Sequential()\n",
    "    result.add_module('conv3x3', nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                                        kernel_size=3, stride=stride, padding=padding, groups=groups,bias=False))\n",
    "    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))\n",
    "    result.add_module('conv1x1', nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                                        kernel_size=1, stride=stride, padding=padding, groups=groups,bias=False))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class RepVGGBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "        super(self).__init__()\n",
    "        self.identity = nn.Identity()\n",
    "        self.block1 = conv_bn()\n",
    "        self.block2 = conv_bn()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.block1(x)\n",
    "        x += self.block2(res)\n",
    "        x += self.identity(res)\n",
    "        x = re\n",
    "class DWConv(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(channel_in, 1, kernel_size, kernel_size))\n",
    "        self.weight_permute = nn.Parameter(torch.randn(channel_out, channel_in, 1, 1))\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
